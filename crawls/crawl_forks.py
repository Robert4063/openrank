"""
Forkæ•°æ®çˆ¬è™«è„šæœ¬
ä½¿ç”¨GitHub APIçˆ¬å–top300é¡¹ç›®æ¯å¤©çš„forkæ•°é‡
åŠŸèƒ½:
- æ–­ç‚¹ç»­ä¼ æ”¯æŒï¼ˆç»­ä¼ æ—¶åˆ é™¤æœ€åä¸€é¡µæ•°æ®é‡æ–°çˆ¬å–ï¼Œç¡®ä¿ä¸é‡å¤ä¸é—æ¼ï¼‰
- å¤šTokenè½®æ¢
- è·å–æ¯ä¸ªé¡¹ç›®çš„forksåŠå…¶forkæ—¶é—´
- æŒ‰æ—¥æœŸç»Ÿè®¡æ¯å¤©çš„forkæ•°é‡
- æ•°æ®å­˜å‚¨åˆ° data/fork/ ç›®å½•
"""
import os
import json
import time
import sys
from datetime import datetime, timezone
from collections import defaultdict
import requests
from tqdm import tqdm
from dotenv import load_dotenv

load_dotenv()

# --- Configuration ---
TOKENS = [
    os.getenv("GITHUB_TOKEN_1", "your_github_token_1"),
    os.getenv("GITHUB_TOKEN_2", "your_github_token_2"),
    os.getenv("GITHUB_TOKEN_3", "your_github_token_3"),
    os.getenv("GITHUB_TOKEN_4", "your_github_token_4"),
]
PROJECT_LIST_FILE = "top300_projects_list.txt"
DATA_DIR = "data"
FORK_DIR = os.path.join(DATA_DIR, "fork")
CHECKPOINT_DIR = os.path.join(DATA_DIR, "fork_checkpoint")

START_DATE = datetime(2022,3,1,tzinfo=timezone.utc)
END_DATE = datetime(2023,3,31,23,59,59, tzinfo=timezone.utc)

class GitHubCrawler:
    def __init__(self, tokens):
        self.tokens = tokens
        self.current_token_index = 0
        self.session = requests.Session()
        self._update_headers()
    
    def _update_headers(self):
        token = self.tokens[self.current_token_index % len(self.tokens)]
        self.session.headers.update({
            'Authorization': f'Bearer {token}',
            'Accept': 'application/vnd.github.v3+json',
            'X-GitHub-Api-Version': '2022-11-28'
        })
    
    def switch_token(self):
        self.current_token_index += 1
        self._update_headers()
        print(f"åˆ‡æ¢åˆ°Token {self.current_token_index % len(self.tokens) + 1}")
        return self.current_token_index
    
    def get_rate_limit_info(self):
        url = "https://api.github.com/rate_limit"
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                core = data.get('resources', {}).get('core', {})
                return core.get('remaining', 0), core.get('reset', 0)
        except:
            pass
        return 0, 0
    
    def get_with_retry(self, url, params=None, max_retries=3):
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, params=params, timeout=30)
                
                remaining = int(response.headers.get('X-RateLimit-Remaining', 1))
                if remaining == 0:
                    reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
                    wait_time = max(reset_time - time.time(), 0) + 5
                    print(f"\nRate limitè¾¾åˆ°ï¼Œåˆ‡æ¢token...")
                    old_index = self.current_token_index
                    self.switch_token()
                    if self.current_token_index >= old_index + len(self.tokens):
                        print(f"æ‰€æœ‰tokenéƒ½è¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾… {min(wait_time, 60):.0f} ç§’...")
                        time.sleep(min(wait_time, 60))
                    continue
                
                if response.status_code == 200:
                    return response.json(), response.headers
                elif response.status_code == 403:
                    error_msg = response.json().get('message', '')
                    if 'rate limit' in error_msg.lower():
                        print(f"\n403 Rate limit, åˆ‡æ¢token...")
                        self.switch_token()
                        continue
                    else:
                        print(f"\n403 Forbidden: {error_msg}")
                        return None, None
                elif response.status_code == 404:
                    return None, None
                elif response.status_code == 422:
                    return None, None
                else:
                    print(f"\nHTTP {response.status_code}: {response.text[:200]}")
                    time.sleep(2)
                    
            except requests.exceptions.RequestException as e:
                print(f"\nè¯·æ±‚é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                time.sleep(5)
        
        return None, None

    def get_forks_page(self, owner, repo, page=1, per_page=100):
        url = f"https://api.github.com/repos/{owner}/{repo}/forks"
        params = {'page': page, 'per_page': per_page, 'sort': 'oldest'}
        data, headers = self.get_with_retry(url, params)
        return data, headers


def ensure_dirs():
    if not os.path.exists(FORK_DIR):
        os.makedirs(FORK_DIR)
    if not os.path.exists(CHECKPOINT_DIR):
        os.makedirs(CHECKPOINT_DIR)


def get_projects():
    projects = []
    if not os.path.exists(PROJECT_LIST_FILE):
        print(f"Error: {PROJECT_LIST_FILE} not found.")
        return []
        
    with open(PROJECT_LIST_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: 
                continue
            if 'â†’' in line:
                projects.append(line.split('â†’')[-1].strip())
            else:
                projects.append(line)
    return projects


def get_safe_name(repo_name):
    return repo_name.replace('/', '_')


def get_checkpoint_path(repo_name):
    safe_name = get_safe_name(repo_name)
    return os.path.join(CHECKPOINT_DIR, f"{safe_name}.json")


def get_output_path(repo_name):
    safe_name = get_safe_name(repo_name)
    return os.path.join(FORK_DIR, f"{safe_name}.json")


def read_checkpoint(repo_name):
    path = get_checkpoint_path(repo_name)
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {
        "last_page": 0, 
        "daily_forks": {}, 
        "page_dates": {},
        "total_forks": 0,
        "completed": False
    }


def write_checkpoint(repo_name, checkpoint_data):
    path = get_checkpoint_path(repo_name)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)


def remove_page_data(daily_forks, page_dates, page):
    page_key = str(page)
    if page_key in page_dates:
        dates_to_remove = page_dates[page_key]
        for date_str in dates_to_remove:
            if date_str in daily_forks:
                pass
        del page_dates[page_key]
    return daily_forks, page_dates

def save_result(repo_name, daily_forks, total_forks):
    path = get_output_path(repo_name)
    sorted_dates = sorted(daily_forks.keys())
    result = {
        "project": repo_name,
        "total_forks_in_range": sum(daily_forks.values()),
        "total_forks_all_time": total_forks,
        "start_date": START_DATE.strftime("%Y-%m-%d"),
        "end_date": END_DATE.strftime("%Y-%m-%d"),
        "crawled_at": datetime.now(timezone.utc).isoformat(),
        "daily_forks": {date: daily_forks[date] for date in sorted_dates}
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def process_repo(crawler, repo_name):
    parts = repo_name.split('/')
    if len(parts) != 2:
        print(f"âš ï¸  è·³è¿‡æ— æ•ˆé¡¹ç›®æ ¼å¼: {repo_name}")
        return False
    
    owner, repo = parts
    
    checkpoint = read_checkpoint(repo_name)
    
    if checkpoint.get("completed", False):
        print(f"[{repo_name}] å·²å®Œæˆï¼Œè·³è¿‡")
        return True
    
    last_page = checkpoint.get("last_page", 0)
    
    daily_forks = defaultdict(int)
    
    # è¯»å– page_data: è®°å½•æ¯ä¸€é¡µæ¯å¤©çš„forkæ•°é‡
    # ç»“æ„: {"page_number": {"date": count, ...}, ...}
    page_data = checkpoint.get("page_data", {})
    total_forks = checkpoint.get("total_forks", 0)
    
    if last_page > 0:
        last_page_key = str(last_page)
        if last_page_key in page_data:
            print(f"[{repo_name}] æ–­ç‚¹ç»­ä¼ ï¼šåˆ é™¤ç¬¬ {last_page} é¡µæ•°æ®å¹¶é‡æ–°çˆ¬å–...")
            # è®¡ç®—éœ€è¦å‡å»çš„ fork æ•°é‡
            removed_count = sum(page_data[last_page_key].values())
            total_forks -= removed_count
            del page_data[last_page_key]
        # ä» last_page é‡æ–°å¼€å§‹ï¼ˆè€Œä¸æ˜¯ last_page + 1ï¼‰
        start_page = last_page
    else:
        start_page = 1
    
    # ä»å·²ä¿å­˜çš„ page_data é‡å»º daily_forks
    for page_key, date_counts in page_data.items():
        for date_str, count in date_counts.items():
            daily_forks[date_str] += count
    
    print(f"[{repo_name}] å¼€å§‹çˆ¬å–ï¼Œä»ç¬¬ {start_page} é¡µå¼€å§‹...")
    
    page = start_page
    forks_in_range = sum(daily_forks.values())
    
    pbar = tqdm(desc=f"[{repo_name}]", unit=" pages", initial=start_page - 1)
    
    try:
        while True:
            data, headers = crawler.get_forks_page(owner, repo, page=page, per_page=100)
            
            if data is None or len(data) == 0:
                break
            
            current_page_data = defaultdict(int)
            
            for fork_info in data:
                total_forks += 1
                
                created_at_str = fork_info.get('created_at')
                if not created_at_str:
                    continue
                
                try:
                    created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
                except:
                    continue
                
                if START_DATE <= created_at <= END_DATE:
                    date_str = created_at.strftime("%Y-%m-%d")
                    daily_forks[date_str] += 1
                    current_page_data[date_str] += 1
                    forks_in_range += 1
            
            page_data[str(page)] = dict(current_page_data)
            
            pbar.update(1)
            
            link_header = headers.get('Link', '') if headers else ''
            if 'rel="next"' not in link_header:
                break
            
            if page % 10 == 0:
                checkpoint_data = {
                    "last_page": page,
                    "daily_forks": dict(daily_forks),
                    "page_data": page_data,
                    "total_forks": total_forks,
                    "completed": False
                }
                write_checkpoint(repo_name, checkpoint_data)
            
            page += 1
            time.sleep(0.1)
        
        pbar.close()
        
        save_result(repo_name, dict(daily_forks), total_forks)
        
        checkpoint_data = {
            "last_page": page,
            "daily_forks": dict(daily_forks),
            "page_data": page_data,
            "total_forks": total_forks,
            "completed": True
        }
        write_checkpoint(repo_name, checkpoint_data)
        
        print(f"[{repo_name}] å®Œæˆ! æ€»fork: {total_forks}, èŒƒå›´å†…: {forks_in_range}")
        return True
        
    except KeyboardInterrupt:
        print(f"\n[{repo_name}] ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...")
        checkpoint_data = {
            "last_page": page,
            "daily_forks": dict(daily_forks),
            "page_data": page_data,
            "total_forks": total_forks,
            "completed": False
        }
        write_checkpoint(repo_name, checkpoint_data)
        raise
    except Exception as e:
        print(f"\n[{repo_name}] é”™è¯¯: {e}")
        checkpoint_data = {
            "last_page": page,
            "daily_forks": dict(daily_forks),
            "page_data": page_data,
            "total_forks": total_forks,
            "completed": False
        }
        write_checkpoint(repo_name, checkpoint_data)
        return False

def main():
    print("=" * 60)
    print("ğŸ´ GitHub Forkæ•°æ®çˆ¬è™« (æ¯æ—¥ç»Ÿè®¡)")
    print("=" * 60)
    print(f"\nğŸ“ é¡¹ç›®åˆ—è¡¨: {PROJECT_LIST_FILE}")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {FORK_DIR}")
    print(f"ğŸ“ æ–­ç‚¹ç›®å½•: {CHECKPOINT_DIR}")
    print(f"ğŸ”‘ Tokenæ•°é‡: {len(TOKENS)}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"âš ï¸  æ–­ç‚¹ç»­ä¼ : è‡ªåŠ¨åˆ é™¤æœ€åä¸€é¡µæ•°æ®å¹¶é‡æ–°çˆ¬å–ï¼Œç¡®ä¿ä¸é‡å¤ä¸é—æ¼")

    ensure_dirs()

    if len(sys.argv) > 1:
        projects = [sys.argv[1]]
    else:
        projects = get_projects()
    
    if not projects:
        print("âŒ æœªæ‰¾åˆ°é¡¹ç›®åˆ—è¡¨")
        return
    
    print(f"\nğŸ“‹ æ‰¾åˆ° {len(projects)} ä¸ªé¡¹ç›®")

    crawler = GitHubCrawler(TOKENS)

    remaining, reset = crawler.get_rate_limit_info()
    print(f"ğŸ“Š å½“å‰Tokenå‰©ä½™è¯·æ±‚æ¬¡æ•°: {remaining}")
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–...\n")
    
    success_count = 0
    error_count = 0
    skipped_count = 0
    
    for i, repo_name in enumerate(projects):
        print(f"\n[{i+1}/{len(projects)}] å¤„ç†: {repo_name}")

        checkpoint = read_checkpoint(repo_name)
        if checkpoint.get("completed", False):
            print(f"  âœ“ å·²å®Œæˆï¼Œè·³è¿‡")
            skipped_count += 1
            continue
        
        try:
            success = process_repo(crawler, repo_name)
            if success:
                success_count += 1
            else:
                error_count += 1
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­!")
            break
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            error_count += 1
            crawler.switch_token()

    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
    print("=" * 60)
    print(f"æ€»é¡¹ç›®æ•°: {len(projects)}")
    print(f"æˆåŠŸ: {success_count}")
    print(f"è·³è¿‡(å·²å®Œæˆ): {skipped_count}")
    print(f"å¤±è´¥: {error_count}")
    print("\nâœ… çˆ¬å–å®Œæˆ!")

if __name__ == "__main__":
    main()
